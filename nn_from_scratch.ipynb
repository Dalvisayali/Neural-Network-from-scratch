{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e989cf1f",
   "metadata": {},
   "source": [
    "# Neural Networks from scratch in Python\n",
    "\n",
    "Neural networks are powerful machine learning models that have revolutionized the field of artificial intelligence. \n",
    "A neural network can be thought of as the functional unit of deep learning, which mimics the behavior of the human brain to solve complex data-driven problems. They can be used to solve a wide range of problems, including image and speech recognition, natural language processing, and predictive modeling. \n",
    "But how does the neural networks actually work? Do they really mimic our brain neurons? How?\n",
    "\n",
    "In this article we will talk about the basic construction of a neural networks and learn how mathematics help this wonder to actually come into reality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee11f9d",
   "metadata": {},
   "source": [
    "## What is an Artificial Neural Network?\n",
    "Artificial neural network is a machine learning algorithm inspired by the human brain that learns from data by processing information through layers of interconnected neurons. This process is similar to how humans learn, where mistakes are made and corrected through training to improve performance. Mathematically, a neural network can be represented as a complex function that maps input variables to output variables, with its parameters optimized through training to minimize a loss function.\n",
    "\n",
    "## Intuition\n",
    "Let's say that you are learning a new sport. Firstly you will take the beginners training and get familiar with the basic rules of the sport. Similarly, a neural network is also trained based on the basic parameters and dataset given to it. This is called Feed Forward. Now once your training is done, your coach makes you play a game. During your first game you actually understand how the game should be played by observing other players and yourself. You evaluate the game play by spotting the differences between your actions and other players, who are actually skilled at the sport. Similarly our neural network also calculates how far our prediction is from the actual result and tries to correct it by adjusting the parameters. This is called Back Propagation, where performance evaluation and error correction is done. Lastly, you will become a pro player only by practising the game. Our neural network also repeats this iterative process of feed forward and back propagtion until it predicts the output correctly. So the ultimate goal is to minimize the error and increase the accuracy by training continuously.\n",
    "\n",
    "Just like a beginner athlete who practices and repeats their movements until they become automatic, a neural network needs to be trained on a large dataset repeatedly to learn patterns and relationships between inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0569b5d",
   "metadata": {},
   "source": [
    "## The Network\n",
    "A neural network is composed of multiple layers of interconnected neurons. By stacking multiple layers of neurons together, a neural network can learn to model complex functions that map input data features to output predictions.\n",
    "Let's try to predict the output of an XOR gate. \n",
    "<br />\n",
    "<center>\n",
    "<img src=\"2-layer-ann.png\" width=400 />\n",
    "</center>\n",
    "\n",
    "Consider the below neural network. It contains an input layer, 1 hidden layer and an output layer. The input layer contains 3 neurons, the hidden layer contains 4 and the final output layer contains a single neuron. \n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ann-1-iniput.png\" width=400 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5462e107",
   "metadata": {},
   "source": [
    "## 1. Neurons\n",
    "Every neuron is a mathematical function that accepts inputs and produces an output that is passed on to other neurons in the next layer. The output is produced based on certain parameters called weights and biases. \n",
    "The input to each neuron is the weighted sum of datapoints and the bias $Wx + b$.\n",
    "Let's define a class with these basic components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f517a63d",
   "metadata": {},
   "source": [
    "## 2. Weights and Biases\n",
    "Weights define the importance or contribution of that particular feature in the output. We have 2 set of weights, one between input layer and the hidden layer and another between hidden and output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f938bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
    "        self.weights2   = np.random.rand(4,1)                 \n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8aa0e1",
   "metadata": {},
   "source": [
    "## 3. Activation Function\n",
    "An activation function is a mathematical function that is applied to the output of a neuron in a neural network. There are several activation functions like sigmoid, hyperbolic tangent, ReLu and the choice depends on the nature of the problem. We will use the *sigmoid* function in our example. It turns out to be a good approximation of the real activation function in human neurons. \n",
    "The sigmoid function is defined as\n",
    "$$ \\sigma(x) = \\dfrac{1} {1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb232599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+ np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515bfdf",
   "metadata": {},
   "source": [
    "## 4. Feedforward\n",
    "If the input is $x$, the output ŷ of a simple 2-layer Neural Network like the one above is, for each of our two layers:\n",
    "\n",
    "$${y_1} = \\sigma(W_1x + b_1)$$\n",
    "\n",
    "$$\\hat{y} = \\sigma\\;(W_2{y_1} + b_2)$$\n",
    "\n",
    "Here, $y_1$ is the output of first layer which is being passed as an input to the second layer.\n",
    "**W1 and W2** are the weights and **b1 and b2** are the baises.\n",
    "Combining the two equations above:\n",
    "\n",
    "$$\\hat{y} = \\sigma\\;(W_2\\sigma(W_1x + b_1) + b_2)$$\n",
    "\n",
    "For simplicity we will ignore the baises. So we are now left with\n",
    "\n",
    "$$\\hat{y} = \\sigma\\;(W_2\\sigma(W_1x))$$\n",
    "\n",
    "This is the initial coaching of our neural network. Now it's time for error correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2da97a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7662c668",
   "metadata": {},
   "source": [
    "## 5. Loss Function\n",
    "The loss function, also called as cost function, is a mathematical function that measures the difference between the predicted output of a neural network and the actual output, also known as the ground truth. It evaluates the performance of the model and indicates the adjustments to be done in weights and baises to increase the accuracy.\n",
    "The goal is to find the optimum set of weights and baises in order to minimize the loss function. The choice of loss function depends on the category of the problem. Here, we’ll use a simple **sum-of-squares** error as our loss function: The sum-of-squares error is simply the sum of the difference between each predicted observation $\\hat{y}$ and the actual observation $y$. The difference is squared so that we measure the *absolute value* of the difference:\n",
    "\n",
    "$$\\text{sum-of-squares-error (L)} = \\sum_{j=\\text{observations}} (y_j - \\hat{y_j})^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e226936f",
   "metadata": {},
   "source": [
    "## 6. Backpropagation\n",
    "Here comes the important step! Now we that we have our measure of goodness of predictions, that is the loss function, we will update the weights and baises with the goal of minimising the error. For this we will take the derivative (gradient descent) of the loss function with respect to weights and baises.\n",
    "The gradient descent gives the slope of the function and tells us how far are we from the minima. \n",
    "So we will update our weights (and biases) thusly:\n",
    "\n",
    "$$w \\rightarrow w + \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "$$b \\rightarrow b + \\frac{\\partial L}{\\partial b}$$\n",
    "where $\\partial$ is the partial derivative and $\\text{L} = \\text{Loss}(y, \\hat{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8c7112",
   "metadata": {},
   "source": [
    "## 7. The Math!\n",
    "Since we have *two* sets of weights, we need *two* derivations of $\\text{Loss}(y, \\hat{y})$:\n",
    "\n",
    "$$\\frac{\\partial \\;\\text{Loss}(y, \\hat{y})}{\\partial W^2}$$\n",
    "\n",
    "$$\\frac{\\partial \\;\\text{Loss}(y, \\hat{y})}{\\partial W^1}$$\n",
    "\n",
    "Loss function is dependant on y and $\\hat{y}$ and not on the weights. But y and $\\hat{y}$ are itself functions of weights. So we can rewrite the equations as:\n",
    "\n",
    "$$\\frac{\\partial \\;\\text{Loss}(y, \\hat{y})}{\\partial W^2} = \n",
    "\\frac{\\partial \\;\\text{Loss}(y, \\hat{y})}{\\partial \\hat{y}} * \\frac{\\partial\\hat{y}}{\\partial W^2} $$\n",
    "\n",
    "$$\\frac{\\partial \\;\\text{Loss}(y, \\hat{y})}{\\partial W^1} = \n",
    "\\frac{\\partial \\;\\text{Loss}(y, \\hat{y})}{\\partial \\hat{y}}* \\frac{\\partial\\hat{y}}{\\partial z} * \\frac{\\partial z}{\\partial W^1} $$\n",
    "\n",
    "Given that,\n",
    "$$z = \\sigma(W^1 x)$$\n",
    "\n",
    "$$\\hat{y} = \\sigma(W^2 z)$$\n",
    "\n",
    "Let's solve for the common term first,\n",
    "\n",
    "$$ \\frac{\\partial \\;\\text{Loss}(y, \\hat{y})}{\\partial \\hat{y}} \n",
    "= \\dfrac{\\sum_{i=1}^{n} (y - \\hat{y})^2} {\\partial \\hat{y}} \n",
    "= \\sum_i \\; 2(y - \\hat{y})$$\n",
    "\n",
    "Substituting this and the equations for $z$ and $\\hat{y}$ we get:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial \\;\\text{Loss}(y, \\hat{y})}{\\partial W^2} = \n",
    "\\sum_i \\; 2(y - \\hat{y}) * \\frac{\\partial \\;\\sigma(W^2 z)}{\\partial W^2}$$\n",
    "\n",
    "$$\\frac{\\partial \\;\\text{Loss}(y, \\hat{y})}{\\partial W^1} = \n",
    "\\sum_i \\; 2(y - \\hat{y}) * \\frac{\\partial\\;\\sigma(W^2 z)}{\\partial z} * \\frac{\\partial \\;\\sigma(W^1 x)}{\\partial W^1}$$\n",
    "\n",
    "\n",
    "In order to take the derivatives we will apply chain rule.\n",
    "\n",
    "The chain rule is stated as: $ \\dfrac{d}{dx}\\left[f(g(x))\\right]  = f'(g(x)) * g'(x) $\n",
    "\n",
    "So with the change of variable $W^2 \\rightarrow q$:\n",
    "\n",
    "$$\\frac{\\partial \\;\\sigma(W^2 z)}{\\partial W^2} = \\frac{\\partial \\;\\sigma(zq)}{\\partial q} = \n",
    "\\sigma'(zq) * \\frac{\\partial \\;(zq)}{\\partial q} = \\sigma'(zq) * z$$\n",
    "\n",
    "$$\\frac{\\partial \\;\\sigma(W^2 z)}{\\partial z} = W^2 * \\sigma'(W^2 z)$$\n",
    "\n",
    "So the first equation becomes:\n",
    "\n",
    "$$\\frac{\\partial \\;\\text{Loss}(y, \\hat{y})}{\\partial W^2} = \n",
    "\\sum_i \\; 2(y - \\hat{y}) * \\sigma'(W^2 z) * z$$\n",
    "\n",
    "Similarly changing variable $W^1 \\rightarrow q$,\n",
    "\n",
    "$$\\frac{\\partial \\;\\sigma(W^1 x)}{\\partial W^1} = \\frac{\\partial \\;\\sigma(xq)}{\\partial q} = \n",
    "\\sigma'(xq) * \\frac{\\partial \\;(xq)}{\\partial q} = \\sigma'(xq) * x$$\n",
    "\n",
    "\n",
    "\n",
    "And the second equation:\n",
    "\n",
    "$$\\frac{\\partial \\;\\text{Loss}(y, \\hat{y})}{\\partial W^1} = \n",
    "\\sum_i \\; 2(y - \\hat{y}) * \\sigma'(W^2 z) * W^2 * \\sigma'(W^1 x) * x$$\n",
    "\n",
    "Now that we have that, let’s add the backpropagation function into our python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61e90e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate the deivative of sigmoid activation function\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
    "\n",
    "def backpropagation(self):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "        sigmoid_derivative_1 = sigmoid_derivative(np.dot(self.input, self.weights1)) #sigma'(W1 x)\n",
    "        sigmoid_derivative_2 = sigmoid_derivative(np.dot(self.layer1, self.weights2)) #sigma'(W2 z)\n",
    "        d_weights2 = np.dot(self.layer1.T, \n",
    "                            (2*(self.y - self.output) * sigmoid_derivative_2))\n",
    "        d_weights1 = np.dot(self.input.T,  \n",
    "                            np.dot(2*(self.y - self.output) * sigmoid_derivative_2, self.weights2.T) * \n",
    "                            sigmoid_derivative_1)\n",
    "\n",
    "        # update the weights with the derivative (slope) of the loss function\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e6534b",
   "metadata": {},
   "source": [
    "## 8. Training\n",
    "Let's put everything together into a class and train our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "155caae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
    "        self.weights2   = np.random.rand(4,1)                 \n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "\n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "        return self.calculate_loss()\n",
    "        \n",
    "    def reload(self, x):\n",
    "        self.input = x\n",
    "        \n",
    "    def predict(self):\n",
    "        return self.output\n",
    "    \n",
    "    def calculate_loss(self):\n",
    "        return (self.y - self.output) ** 2\n",
    "    \n",
    "    def plot_loss(self,loss):\n",
    "        '''\n",
    "        Plots the loss curve\n",
    "        '''\n",
    "        plt.plot(loss)\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"logloss\")\n",
    "        plt.title(\"Loss curve for training\")\n",
    "        plt.show()  \n",
    "\n",
    "    def backprop(self):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "#         print(\"Loss = \"+str(self.calculate_loss()))\n",
    "        sigmoid_derivative_1 = sigmoid_derivative(np.dot(self.input, self.weights1)) #sigma'(W1 x)\n",
    "        sigmoid_derivative_2 = sigmoid_derivative(np.dot(self.layer1, self.weights2)) #sigma'(W2 z)\n",
    "        d_weights2 = np.dot(self.layer1.T, \n",
    "                            (2*(self.y - self.output) * sigmoid_derivative_2))\n",
    "        d_weights1 = np.dot(self.input.T,  \n",
    "                            np.dot(2*(self.y - self.output) * sigmoid_derivative_2, self.weights2.T) * \n",
    "                            sigmoid_derivative_1)\n",
    "\n",
    "        # update the weights with the derivative (slope) of the loss function\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985a97b1",
   "metadata": {},
   "source": [
    "We are training on XOR dataset. So our X and y will be as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8d6e2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 3), (4, 1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0,0,1],\n",
    "              [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c6f0d",
   "metadata": {},
   "source": [
    "Let's train!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f6ac652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00854594]\n",
      " [0.96724274]\n",
      " [0.96728283]\n",
      " [0.03872196]]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(X,y)\n",
    "loss = []\n",
    "for i in range(1500):\n",
    "    loss.append(nn.feedforward())\n",
    "    nn.backprop()\n",
    "print(nn.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cd014e",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "If we plot the Loss at each iteration, we find:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ann-1-loss.png\" width=600 />\n",
    "</center>\n",
    "\n",
    "Let’s look at the final prediction (output) from the Neural Network after 1500 iterations.\n",
    "Predictions after 1500 training iterations\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ann-1-predictions.png\" width=200 />\n",
    "</center>\n",
    "\n",
    "Congratulations, you now have a fully functional, 2-layer neural network for a binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa70a760",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
